{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m \u001b[39mimport\u001b[39;00m Chrome\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchrome\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptions\u001b[39;00m \u001b[39mimport\u001b[39;00m Options\n\u001b[1;32m      7\u001b[0m headers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "\n",
    "greenhouse = \"https://boards.greenhouse.io\"\n",
    "\n",
    "jobs = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Adobe jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adobe_pages = ''\n",
    "adobe_url = 'https://careers.adobe.com/us/en/c/engineering-and-product-jobs?'+adobe_pages+'s=1'\n",
    "print(adobe_url)\n",
    "adobe_response = requests.get(adobe_url, headers=headers)\n",
    "\n",
    "adobe_soup = BeautifulSoup(adobe_response.content, 'html5lib')\n",
    "print(adobe_soup.prettify())\n",
    "\n",
    "# write output to a file\n",
    "with open('adobe.html', 'w') as f:\n",
    "    f.write(adobe_soup.prettify())\n",
    "\n",
    "# adobe_jobs_count = adobe_soup.select('span.result-count')[0].string\n",
    "# adobe_jobs_count = adobe_soup.find('span', class_='result-count').string\n",
    "# print(adobe_jobs_count)\n",
    "# adobe_pages_count = int(adobe_jobs_count/10)\n",
    "\n",
    "adobe_jobs = adobe_soup.find_all('li', class_='jobs-list-item')\n",
    "\n",
    "for i in range(1, adobe_pages_count):\n",
    "    adobe_pages = 'from='+i*10+'&'\n",
    "    adobe_response = requests.get(adobe_url)\n",
    "    adobe_soup = BeautifulSoup(adobe_response.text, 'html.parser')\n",
    "    adobe_jobs += adobe_soup.find_all('li', class_='jobs-list-item')\n",
    "\n",
    "# get the job title from the adobe_jobs list stored in anchor tag in attribute aria-label\n",
    "adobe_jobs_title = [job.find('a')['aria-label'] for job in adobe_jobs] \n",
    "\n",
    "# get job location from attribute data-ph-at-job-location-text\n",
    "adobe_jobs_location = [job.find('a')['data-ph-at-job-location-text'] for job in adobe_jobs]\n",
    "\n",
    "# get job link from attribute href\n",
    "adobe_jobs_link = [job.find('a')['href'] for job in adobe_jobs]\n",
    "\n",
    "# get posted date from span.job-postdate \n",
    "adobe_jobs_posted = [job.find('span', class_='job-postdate').text for job in adobe_jobs]\n",
    "\n",
    "# get job description from p.job-description innertext\n",
    "adobe_jobs_description = [job.find('p', class_='job-description').text for job in adobe_jobs]\n",
    "\n",
    "# get job id from span.jobId innertext\n",
    "adobe_jobs_id = [job.find('span', class_='jobId').text for job in adobe_jobs]\n",
    "\n",
    "# print(adobe_jobs_title)\n",
    "\n",
    "# write results into a json file named as jobs.json in objects list with each object containing job title, location, link, posted date, description, id, key (where key increments by 1) and company name\n",
    "\n",
    "for i in range(len(adobe_jobs_title)):\n",
    "    jobs.append({\n",
    "        'title': adobe_jobs_title[i],\n",
    "        'location': adobe_jobs_location[i],\n",
    "        'link': adobe_jobs_link[i],\n",
    "        'posted': adobe_jobs_posted[i],\n",
    "        'description': adobe_jobs_description[i],\n",
    "        'id': adobe_jobs_id[i],\n",
    "        'company': 'Adobe'\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Able jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "able_url = 'https://boards.greenhouse.io/able'\n",
    "able_response = requests.get(able_url, headers=headers)\n",
    "able_soup = BeautifulSoup(able_response.content, 'html5lib')\n",
    "# print(able_soup.prettify())\n",
    "\n",
    "able_jobs = able_soup.find_all('a', attrs={\"data-mapped\":\"true\"})\n",
    "# print(able_jobs)\n",
    "\n",
    "# get the job title from able_jobs list stored in text\n",
    "able_jobs_title = [job.text for job in able_jobs]\n",
    "# print(able_jobs_title)\n",
    "\n",
    "# get job link from attribute href\n",
    "able_jobs_link = [greenhouse+job['href'] for job in able_jobs]\n",
    "# print(able_jobs_link)\n",
    "\n",
    "# get job location from span.location innertext\n",
    "able_jobs_location = [able_soup.find_all('span', class_=\"location\")[i].text for i in range(len(able_jobs))]\n",
    "# print(able_jobs_location)\n",
    "\n",
    "# writing results into a json file named as jobs.json in objects list with each object containing job title, location, link, posted date, description, id and key(we will use index of jobs[]).\n",
    "\n",
    "for i in range(len(able_jobs_title)):\n",
    "    jobs.append({\n",
    "        'title': able_jobs_title[i],\n",
    "        'location': able_jobs_location[i],\n",
    "        'link': able_jobs_link[i],\n",
    "        'posted': '',\n",
    "        'description': '',\n",
    "        'id': '',\n",
    "        'company': 'Able'\n",
    "    })\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Affinidi jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m affinidi_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.affinidi.com/careers\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# use selenium to get the page source\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m options \u001b[39m=\u001b[39m Options()\n\u001b[1;32m      5\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--headless\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m driver \u001b[39m=\u001b[39m Chrome(options\u001b[39m=\u001b[39moptions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Options' is not defined"
     ]
    }
   ],
   "source": [
    "affinidi_url = 'https://www.affinidi.com/careers'\n",
    "\n",
    "# use selenium to get the page source\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = Chrome(options=options)\n",
    "driver.get(affinidi_url)\n",
    "\n",
    "affinidi_soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "\n",
    "# get span.location innertext\n",
    "affinidi_jobs_location = [affinidi_soup.find_all('span', class_=\"location\")[i].text for i in range(len(affinidi_soup.find_all('span', class_=\"location\")))]\n",
    "print(affinidi_jobs_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
